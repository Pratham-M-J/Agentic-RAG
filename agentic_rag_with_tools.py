# -*- coding: utf-8 -*-
"""Agentic_RAG_with_tools.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cI3OqJubaPBBipLBznDgFphJq9sKCyUQ
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install llama-index

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install llama-index-llms-gemini

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install llama-index-vector-stores
# %pip install llama-index-embeddings-huggingface

from llama_index.core.tools import FunctionTool
from llama_index.llms.gemini import Gemini

model = Gemini(
    model = 'gemini-2.0-flash',
    api_key = 'YOUR_API_KEY_HERE'
)

from llama_index.embeddings.huggingface import HuggingFaceEmbedding
embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")

from llama_index.core import Settings
Settings.llm = model
Settings.embed = embed_model

from llama_index.core.tools import FunctionTool
def add(x:int, y:int)->int:
  """Adds two numbers"""
  return x + y

def mystery(x:int, y:int)->int:
  """Mystery function operates on top of two numbers"""
  return (x + y) * (x + y)

add_tool = FunctionTool.from_defaults(fn=add)
mystery_tool = FunctionTool.from_defaults(fn=mystery)

response = model.predict_and_call(
    [add_tool, mystery_tool],
    "heyy! mystery of 5 and 5 and add 6 and 7",
    verbose = True,
    allow_parallel_tool_calls = True
)

print(str(response))

from llama_index.core import SimpleDirectoryReader

input_file = input("Enter the file name (.pdf): ")
documents = SimpleDirectoryReader(input_files = [input_file]).load_data()

from llama_index.core.node_parser import SentenceSplitter

splitter = SentenceSplitter(
    chunk_size = 1024,
    chunk_overlap = 20
)

chunks = splitter.get_nodes_from_documents(documents)

chunks[1]

print(chunks[0].get_content(metadata_mode="all"))

Settings.llm = model
Settings.embed_model = embed_model

from llama_index.core import VectorStoreIndex

vector_index = VectorStoreIndex(chunks)
vector_store_engine = vector_index.as_query_engine(similarity_top_k = 2)

from typing import List
from llama_index.core.vector_stores import FilterCondition

def vector_query(query: str)->str:
  """
  Perform a vector search over an index.

  query (str): The dtring query to the embedded.
  page_numbers (List[str]): Filter by set of pages. Leave BLANK if we wnat to perform a vector search
  over all pages, Otherwise, filter by the set of specified pages.
  """
  query_engine = vector_index.as_query_engine(
      similarity_top_k = 2
  )
  response = query_engine.query(query)
  return response

print(vector_query("is gradio used?"))

vector_query_tool = FunctionTool.from_defaults(
    name= "vector_tool",
    fn = vector_query,
    description = "Useful for answering questions by searching through the uploaded document content using vector search. Use this tool for any question that requires retrieving information from the document."
)

response = model.predict_and_call(
    [vector_query_tool],
    "Call the available tools with query='what is gradio used for?'",
    verbose=True
)

response.response

from llama_index.core import SummaryIndex
from llama_index.core.tools import QueryEngineTool

summary_index = SummaryIndex(chunks)


def summarize(query:str)->str:
  """
  Perform a search over an index.

  query (str): The string query to the embedded.
  page_numbers (List[str]): Filter by set of pages. Leave BLANK if we wnat to perform a vector search
  over all pages, Otherwise, filter by the set of specified pages.
  """
  query_engine = summary_index.as_query_engine(
      response_mode = "tree_summarize",
      use_async = True,
  )
  response = query_engine.query(query)
  return response

summary_tool = FunctionTool.from_defaults(
    fn = summarize,
    name = "summary_tool",
    description = "Useful for summarization questions."
)

response = model.predict_and_call(
    [vector_query_tool, summary_tool,add_tool,mystery_tool],
    "decide and call the available tool based on the query: add 5 and 6 then perform mystery calculation of the result of addition with 9.",
    verbose=True,
    allow_parallel_tool_calls=True
)
response.response

